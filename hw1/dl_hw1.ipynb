{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eric He\n",
    "### eh1885\n",
    "### Deep Learning\n",
    "### Homework 1: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Two-layer neural network\n",
    "\n",
    "## 1.1 Regression task\n",
    "\n",
    "### 1.1.a Name the 5 programming steps you would take to train this model with PyTorch using SGD on a single batch of data\n",
    "Suppose we are provided `model`, a Torch model object, a loss function given by `l(y_pred, y_true)`, and a learning rate given by `lr`.\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "y_pred = model.forward() # 1. forward pass \n",
    "loss = l(y_pred, y_true) # 2. compute loss\n",
    "optimizer.zero_grad() # 3. zero the grad-parameters\n",
    "loss.backward() # 4. accumulate the grad-parameters\n",
    "optimizer.step() # 5. step in the opposite direction of the grad-parameters\n",
    "```\n",
    "\n",
    "### 1.1.b For a single data point `(x, y)`, write down all inputs and outputs for forward pass of each layer. You can only use `x, y, W^1, b^1, W^2, b^2` in your answer.\n",
    "\n",
    "#### Linear 1\n",
    "Input: $x$\n",
    "\n",
    "Output: $W^1x + b^1$\n",
    "\n",
    "#### f\n",
    "Input: $W^1x + b^1$\n",
    "\n",
    "Output: $f(W^1x + b^1)$\n",
    "\n",
    "#### Linear 2\n",
    "Input: $f(W^1x + b^1)$\n",
    "\n",
    "Output: $W^2(f(W^1x + b^1)) + b^2$\n",
    "\n",
    "#### g\n",
    "Input: $W^2(f(W^1x + b^1)) + b^2$\n",
    "\n",
    "Output: $g(W^2(f(W^1x + b^1)) + b^2)$\n",
    "\n",
    "#### Loss\n",
    "Input: $g(W^2(f(W^1x + b^1)) + b^2)$\n",
    "\n",
    "Output: $\\dfrac{1}{2}(g(W^2(f(W^1x + b^1)) + b^2) - y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.c Write down the gradient calculated from the backward pass. You can only use $x, y, W^1, b^1, W^2, b^2, \\dfrac{\\delta l}{\\delta y}, \\dfrac{\\delta z_2}{\\delta z_1}, \\dfrac{\\delta \\hat{y}}{\\delta z_3}$ in your answer, where $z_1, z_2, z_3, \\hat{y}$ are the outputs of `Linear_1, f, Linear_2, g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, assume $W^1$ is a matrix of shape $(m_1, n_1)$ and assume $W^2$ is a matrix of shape $(m_2, n_2)$. Denote $I_{m_1}$ to be the identity matrix of shape $(m_1, m_1)$ and $I_{m_2}$ to be the identity matrix of shape $(m_2, m_2)$.\n",
    "\n",
    "#### W1\n",
    "We first expand the derivative:\n",
    "$$\n",
    "\\dfrac{\\delta l}{\\delta W^1} = \\dfrac{\\delta l}{\\delta \\hat{y}}\n",
    "    \\dfrac{\\delta \\hat{y}}{\\delta z_3}\n",
    "    \\dfrac{\\delta z_3}{\\delta z_2} \n",
    "    \\dfrac{\\delta z_2}{\\delta z_1}\n",
    "    \\dfrac{\\delta z_1}{\\delta W^1}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to fill in the gradients $\\dfrac{\\delta z_3}{\\delta z_2}$ and $\\dfrac{\\delta z_1}{\\delta W^1}$.\n",
    "\n",
    "As $z_3 = W^2 z_2 + b^2$, we have $\\dfrac{\\delta z_3}{\\delta z_2} = W^2$. \n",
    "\n",
    "As $z_1 = W^1 x + b^1$, we have $\\dfrac{\\delta z_1}{\\delta W^1} = I_{m_1}x^T$. Notice that I have written this gradient as a matrix where $x^T$ is repeated several $m_1$ times; strictly speaking, because $z_1$ is a vector of shape $m_1$ and $W_1$ is a matrix of shape $(m_1, m_2)$, the Jacobian is of shape $(m_1, m_1, m_2)$. However, because $W_1$ is linear, the Jacobian is diagonal along one of the $m_1$ dimensions and I was able to squash it to a matrix with no repercussions.\n",
    "\n",
    "Then the entire computation is \n",
    "\n",
    "$$\n",
    "\\dfrac{\\delta l}{\\delta W^1} = \\dfrac{\\delta l}{\\delta \\hat{y}}\n",
    "    \\dfrac{\\delta \\hat{y}}{\\delta z_3}\n",
    "    W^2\n",
    "    \\dfrac{\\delta z_2}{\\delta z_1}\n",
    "    I_{m_1}x^T\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.d Show us the elements of $\\dfrac{\\delta z_2}{\\delta z_1}, \\dfrac{\\delta \\hat{y}}{\\delta z_3}$ and $\\dfrac{\\delta l}{\\delta \\hat{y}}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\delta z_1}{\\delta z_1}$ is the derivative of the ReLU operation applied elementwise to the vector $z_1$. The subgradient of ReLU(x) is $1$ if $x > 0$ and $0$ otherwise, so we can write $\\dfrac{\\delta z_2}{\\delta z_1} = 1_{z_{1} > 0}$, the indicator function elementwise for if $z_{1i} > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\delta \\hat{y}}{\\delta z_3}$ is the derivative of the identity function of $z_3$. The identity function has derivative $1$, the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\delta l}{\\delta \\hat{y}}$ is the derivative of the square loss, and is given by $\\hat{y} - y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classification Task\n",
    "\n",
    "### 1.2.a If you want to train this network, what do you need to change in the equations of (b), (c), and (d), assuming we are using the same MLE loss function.\n",
    "The forward pass would look the same, of course as I retained the $f$ and $g$ notations in the original answer. However, instead of $f(W^1x + b^1) = \\max(0, W^1x + b^1)$, we now have  $f(W^1x + b^1) = \\sigma(W^1x + b^1)$. And instead of $g(W^2(f(W^1x + b^1)) + b^2) = W^2(f(W^1x + b^1)) + b^2$, $g_2$ is now $\\sigma(W^2(f(W^1x + b^1)) + b^2)$\n",
    "\n",
    "The backward pass now involves the deriving the logistic sigmoid function, $\\sigma(z) = (1 + \\exp(-z))^{-1}$. We have\n",
    "\n",
    "$$\n",
    "\\dfrac{\\delta \\sigma}{\\delta z} = \n",
    "    -(1 + \\exp(-z))^{-2})(-\\exp(-z)) \\\\\n",
    "    = \\dfrac{\\exp(-z)}{(1 + \\exp(-z))^2}\n",
    "$$\n",
    "\n",
    "For $f(z_1)$, this would correspond to $z_1 = W^1 x + b^1$. We would substitute in the above for $\\dfrac{\\delta z_2}{\\delta z_1}$.\n",
    "\n",
    "For $g(z_3)$, this would correspond to $z_3 = W^2(f(W^1x + b^1)) + b^2$. We would substitute in the above for $\\dfrac{\\delta \\hat{y}}{\\delta z_3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.b Now you think you can do a better job by using a binary cross-entropy (BCE) loss function $l_{BCE}(\\hat{y}, y) = -[y\\log(\\hat{y}) + (1-y)\\log(1 - \\hat{y})]$. What do you need to change in the equations of (b), (c) and (d)?\n",
    "In the forward pass, we would only have to change the final loss function. \n",
    "\n",
    "In the backward pass, we would have to change the $\\dfrac{\\delta l}{\\delta \\hat{y}}$ multiple in each of the gradient calculations. The gradient of the cross-entropy loss function with respect to $\\hat{y}$ is given by $-(\\dfrac{y}{\\hat{y}} - \\dfrac{1-y}{1 - \\hat{y}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.c\n",
    "Setting $f$ to be the ReLU function is better for training deeper networks because the ReLU function does not have gradient saturation in the same way the sigmoid function does. The ReLU function's derivative, as claimed earlier, is an indicator function, while the sigmoid function's derivative goes to 0 as the sigmoid's input takes extreme value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
